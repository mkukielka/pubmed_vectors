{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import numpy as np\n",
    "from smart_open import smart_open\n",
    "\n",
    "\n",
    "def process_row(row):\n",
    "    line = row.rstrip().split(' ')\n",
    "    return line[0], line[1:]\n",
    "\n",
    "\n",
    "def load_vocabulary(vocab_file):\n",
    "    with smart_open(vocab_file, 'r+') as vocabulary, Pool(cpu_count()) as p:\n",
    "        for word, _ in p.imap(process_row, vocabulary):\n",
    "            yield word\n",
    "\n",
    "\n",
    "def load_vectors(glove_vectors_file):\n",
    "    with smart_open(glove_vectors_file, 'r+') as glove_vectors, Pool(cpu_count()) as p:\n",
    "        vectors = {}\n",
    "        for word, values in p.imap(process_row, glove_vectors):\n",
    "            vectors[word] = np.float(values)\n",
    "        return vectors\n",
    "\n",
    "\n",
    "def laod_data_and_evaluate(vocab_file, glove_vectors_file, path=''):\n",
    "    words = list(load_data(vocab_file))\n",
    "    vectors = load_vectors(glove_vectors_file)\n",
    "\n",
    "    vocab_size = len(words)\n",
    "    vocab = {w: idx for idx, w in enumerate(words)}\n",
    "    ivocab = {idx: w for idx, w in enumerate(words)}\n",
    "\n",
    "    vector_dim = len(vectors[ivocab[0]])\n",
    "    W = np.zeros((vocab_size, vector_dim))\n",
    "    for word, v in vectors.iteritems():\n",
    "        if word == '<unk>':\n",
    "            continue\n",
    "        W[vocab[word], :] = v\n",
    "\n",
    "    # normalize each word vector to unit variance\n",
    "    W_norm = np.zeros(W.shape)\n",
    "    d = (np.sum(W ** 2, 1) ** (0.5))\n",
    "    W_norm = (W.T / d).T\n",
    "    evaluate_vectors(W_norm, vocab, ivocab, path)\n",
    "\n",
    "\n",
    "def evaluate_vectors(W, vocab, ivocab, path):\n",
    "    \"\"\"Evaluate the trained word vectors on a variety of tasks\"\"\"\n",
    "\n",
    "    # to avoid memory overflow, could be increased/decreased\n",
    "    # depending on system and vocab size\n",
    "    split_size = 100\n",
    "    correct_sem = 0  # count correct semantic questions\n",
    "    correct_syn = 0  # count correct syntactic questions\n",
    "    correct_tot = 0  # count correct questions\n",
    "    count_sem = 0  # count all semantic questions\n",
    "    count_syn = 0  # count all syntactic questions\n",
    "    count_tot = 0  # count all questions\n",
    "    full_count = 0  # count all questions, including those with unknown words\n",
    "\n",
    "    for filename in glob.glob('*.txt'.format(path)):\n",
    "        with open(filename, 'r') as f:\n",
    "            full_data = [line.rstrip().split(' ') for line in f]\n",
    "            full_count += len(full_data)\n",
    "            data = [x for x in full_data if all(word in vocab for word in x)]\n",
    "\n",
    "        indices = np.array([[vocab[word] for word in row] for row in data])\n",
    "        ind1, ind2, ind3, ind4 = indices.T\n",
    "\n",
    "        predictions = np.zeros((len(indices),))\n",
    "        num_iter = int(np.ceil(len(indices) / float(split_size)))\n",
    "        for j in xrange(num_iter):\n",
    "            subset = np.arange(j*split_size, min((j + 1)*split_size, len(ind1)))\n",
    "\n",
    "            pred_vec = (W[ind2[subset], :] - W[ind1[subset], :] +\n",
    "                        W[ind3[subset], :])\n",
    "            # cosine similarity if input W has been normalized\n",
    "            dist = np.dot(W, pred_vec.T)\n",
    "\n",
    "            for k in xrange(len(subset)):\n",
    "                dist[ind1[subset[k]], k] = -np.Inf\n",
    "                dist[ind2[subset[k]], k] = -np.Inf\n",
    "                dist[ind3[subset[k]], k] = -np.Inf\n",
    "\n",
    "            # predicted word index\n",
    "            predictions[subset] = np.argmax(dist, 0).flatten()\n",
    "\n",
    "        val = (ind4 == predictions)  # correct predictions\n",
    "        count_tot = count_tot + len(ind1)\n",
    "        correct_tot = correct_tot + sum(val)\n",
    "        if i < 5:\n",
    "            count_sem = count_sem + len(ind1)\n",
    "            correct_sem = correct_sem + sum(val)\n",
    "        else:\n",
    "            count_syn = count_syn + len(ind1)\n",
    "            correct_syn = correct_syn + sum(val)\n",
    "\n",
    "        print(\"%s:\" % filename)\n",
    "        print('ACCURACY TOP1: %.2f%% (%d/%d)' %\n",
    "              (np.mean(val) * 100, np.sum(val), len(val)))\n",
    "\n",
    "    print('Questions seen/total: %.2f%% (%d/%d)' %\n",
    "          (100 * count_tot / float(full_count), count_tot, full_count))\n",
    "    print('Semantic accuracy: %.2f%%  (%i/%i)' %\n",
    "          (100 * correct_sem / float(count_sem), correct_sem, count_sem))\n",
    "    print('Syntactic accuracy: %.2f%%  (%i/%i)' %\n",
    "          (100 * correct_syn / float(count_syn), correct_syn, count_syn))\n",
    "    print('Total accuracy: %.2f%%  (%i/%i)' %\n",
    "          (100 * correct_tot / float(count_tot), correct_tot, count_tot))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
