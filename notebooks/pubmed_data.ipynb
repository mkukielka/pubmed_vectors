{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import glob\n",
    "import gzip\n",
    "import xml.etree.ElementTree as ET\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import numpy as np\n",
    "from smart_open import smart_open\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def preprocessing(path, corpus_file='corpus_file.txt'):\n",
    "    \"\"\"\n",
    "    Creates corpus based on articles from annual baseline of Medline/Pubmed Database\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : string\n",
    "        Path to folder with files .gz from pubmed\n",
    "    corpus_file: string\n",
    "        Name of file, in which corpus will be created\n",
    "    \"\"\"\n",
    "    with smart_open(corpus_file, mode='w+') as corpus, Pool(cpu_count()) as p:\n",
    "        for article in p.imap(process_article_set, glob.glob('{}*.gz'.format(path))):\n",
    "            corpus.write(article)\n",
    "\n",
    "\n",
    "def process_article_set(file):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    with gzip.open(file) as xml_file:\n",
    "        try:\n",
    "            article_set = ET.parse(xml_file).getroot()\n",
    "            results = ''\n",
    "            for article in article_set:\n",
    "                results += process_article(article, translator)\n",
    "            return results\n",
    "        except ET.ParseError:\n",
    "            return ''\n",
    "\n",
    "\n",
    "def process_article(article, translator):\n",
    "    title = article.find('MedlineCitation/Article/ArticleTitle')\n",
    "    abstract = article.find('MedlineCitation/Article/Abstract/AbstractText')\n",
    "    mesh_heading = process_mesh_heading(article.find('MedlineCitation/MeshHeadingList'))\n",
    "    article_data = ''\n",
    "    if title is not None:\n",
    "        article_data += process_raw_text(title, translator)\n",
    "    if abstract is not None:\n",
    "        article_data += process_raw_text(abstract, translator)\n",
    "    if mesh_heading is not None:\n",
    "        article_data += process_raw_text(mesh_heading, translator)\n",
    "    return '{}\\n'.format(article_data.strip().replace('\\n', ''))\n",
    "\n",
    "\n",
    "def process_raw_text(data, translator):\n",
    "    \"\"\"Removes punctuation and uppercase from given string.\"\"\"\n",
    "    try:\n",
    "        return ' '.join(data.text.lower().translate(translator).split())\n",
    "    except AttributeError:\n",
    "        return ''\n",
    "\n",
    "\n",
    "def process_mesh_heading(data):\n",
    "    \"\"\"Reads meshheadinglist and returns names of descriptors.\"\"\"\n",
    "    return ' '.join(map(lambda x: x.text.lower(), data.findall(\n",
    "        'MeshHeading/DescriptorName'))) if data is not None else ''\n",
    "\n",
    "\n",
    "def length_of_local_context(path):\n",
    "    \"\"\"Finds the length of local context window, which fully covers every single article\"\"\"\n",
    "    with open(path, 'r+') as corpus, Pool(cpu_count()) as p:\n",
    "        results = [cur_length for cur_length in p.imap(count_words, corpus)]\n",
    "        return 'Average length: {} \\n Max: {} \\n Median: {}'.format(\n",
    "            np.mean(results), np.max(results), np.median(results))\n",
    "\n",
    "    \n",
    "def count_words(text):\n",
    "    return len(split_words(text))\n",
    "\n",
    "\n",
    "def split_words(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing nosiy data like most common english words and integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with smart_open('google-10000-english.txt', 'r+') as f:\n",
    "    top_english_words = [word.strip('\\n') for word in f]\n",
    "\n",
    "\n",
    "def cast_to_integer(word):\n",
    "    try:\n",
    "        return int(word)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def process_line(line):\n",
    "    word, counter = line.strip('\\n').split()\n",
    "    if word in top_english_words:\n",
    "        return None\n",
    "    if cast_to_integer(word):\n",
    "        return None\n",
    "    return line \n",
    "\n",
    "\n",
    "with smart_open('vocabulary10.txt', 'r+') as vocab, smart_open('filtered_vocab_10.txt', 'w+') as output, Pool(cpu_count()) as p:\n",
    "    for line in tqdm(p.imap(process_line, vocab)):\n",
    "        if line:\n",
    "            output.write('{}\\n'.format(line))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
